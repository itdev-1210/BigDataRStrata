---
title: 'R with Big Data 6: Spark Extensions'
author: "John Mount"
output:
  md_document:
    variant: markdown_github
---

This is only a concept script, it runs correctly but is intended for
teaching not direct use in production.

At some point the `SparkR:dapply()` functionality we are working to capture here will be available as a method called [`sparklyr::spark_apply()`](https://github.com/rstudio/sparklyr/pull/728).

One point in particular is this script assumes none of the following directories are present (as it is going to try to create them and write its own temp results):

  * Exercises/solutions/df*_tmp
  * Exercises/solutions/tmpFile*_*

We don't delete these as we don't want to perform too many (potentially unsafe) file operations on the user's behalf.



```{r setup}
Sys.setenv(TZ='UTC')
library("sparklyr")
library("dplyr")
sc <- spark_connect(master = "local", version = "2.0.0", hadoop_version="2.7")
```


```{r datesexample}
d <- data_frame(x = c(20100101120101, "2009-01-02 12-01-02", "2009.01.03 12:01:03",
       "2009-1-4 12-1-4",
       "2009-1, 5 12:1, 5",
       "200901-08 1201-08",
       "2009 arbitrary 1 non-decimal 6 chars 12 in between 1 !!! 6",
       "OR collapsed formats: 20090107 120107 (as long as prefixed with zeros)",
       "Automatic wday, Thu, detection, 10-01-10 10:01:10 and p format: AM",
       "Created on 10-01-11 at 10:01:11 PM"))

df  <- copy_to(sc, d, 'df')
print(df)
```

Running `SQL` directly (see [http://spark.rstudio.com](http://spark.rstudio.com)).

```{r sql}
library("DBI")

# returns a in-memor data.frame
dfx <- dbGetQuery(sc, "SELECT * FROM df LIMIT 5")
dfx

# build another table
dbSendQuery(sc, "CREATE TABLE df2 AS SELECT * FROM df LIMIT 5")
# get a handle to it
dbListTables(sc)
df2 <- dplyr::tbl(sc, 'df2')
df2
```


Using `SparkR` for `R` user defined functions.

The following doesn't always run in a knitr evironment.  And using `SparkR` in production would entail already having the needed R packages installed.

```{r sparkr}
# Connect via SparkR, more notes: https://github.com/apache/spark/tree/master/R
SPARK_HOME <- sc$spark_home
# https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/KDDCup2016/Code/SparkR/SparkR_sparklyr_NYCTaxi.Rmd
# http://sbartek.github.io/sparkRInstall/installSparkReasyWay.html
library(SparkR, lib.loc = paste0(SPARK_HOME, "/R/lib/"))
sr <- sparkR.session(master = "local", sparkHome = SPARK_HOME)

sparklyr::spark_write_parquet(df, 'df_tmp')
dSparkR <- SparkR::read.df('df_tmp')
# http://spark.apache.org/docs/latest/sparkr.html
schema <- structType(structField("dateStrOrig", "string"), 
                     structField("dateStrNorm", "timestamp"),
                     structField("dateSec", "double"))
dSparkR2 <- SparkR::dapply(dSparkR, function(x) {
  if(!require('lubridate', quietly = TRUE)) {
    install.packages("lubridate", repos= "http://cran.rstudio.com")
  }
  s <- lubridate::ymd_hms(x[[1]])
  x <- cbind(x, s, as.numeric(s))
  x
  }, schema)
SparkR::write.df(dSparkR2, 'dfR_tmp')
dfR <- sparklyr::spark_read_parquet(sc, 'dfR', 'dfR_tmp')
dfR <- dfR %>% 
  dplyr::mutate(dt = from_unixtime(dateSec)) %>%
  dplyr::select(dateStrNorm, dateSec, dt)
print(dfR)
```




From: [http://spark.rstudio.com/extensions.html](http://spark.rstudio.com/extensions.html).

```{r count}
count_lines <- function(sc, file) {
  spark_context(sc) %>% 
    invoke("textFile", file, 1L) %>% 
    invoke("count")
}

count_lines(sc, "tmp.csv")
```

A simple Java example.

```{r trivialexample}
billionBigInteger <- invoke_new(sc, "java.math.BigInteger", "1000000000")
print(billionBigInteger)
str(billionBigInteger)

billion <- invoke(billionBigInteger, "longValue")
str(billion)
```




